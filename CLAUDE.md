# Claude Code Reference Documentation

This file provides quick navigation to all project documentation for AI-assisted development.

## Project Overview

**Name**: å‰§æœ¬å™äº‹ç»“æ„åˆ†æç³»ç»Ÿ (Script Narrative Structure Analysis System)

**Purpose**: Multi-agent LLM system for analyzing and correcting narrative structure in screenplays using a three-stage pipeline (Discover â†’ Audit â†’ Modify).

**Technology Stack**: Python, LangChain, LangGraph, Pydantic, DeepSeek/Claude/OpenAI

**Current Version**: 2.3.0 (2025-11-13)
**Last Updated**: 2025-11-13
**Completion**: 100% (All three stages verified and operational + LangSmith observability + A/B testing + Markdown export)

---

## Quick Start

### For First-Time Users
1. Read: [`ref/getting-started.md`](ref/getting-started.md) - Installation, setup, and basic usage
2. Run: `pip install -r requirements.txt` - Install dependencies (âœ… Done)
3. Configure: Copy `.env.example` to `.env` and add your DeepSeek API key (âœ… Done)
4. Run: `pytest tests/ -v` - Verify installation (âœ… 44/44 tests passing)
5. Try: `python -m src.cli analyze examples/golden/ç™¾å¦–_ep09_s01-s05.json` (âœ… All stages working)

### For Developers
1. Read: [`ref/architecture.md`](ref/architecture.md) - System design and components
2. Read: [`prompts/README.md`](prompts/README.md) - Prompt engineering guide
3. Review: [`tests/`](tests/) - Test examples

---

## Current Status (2025-11-13)

### âœ… What's Working (100% Complete)
- **Unit Tests**: 44/44 passing (100% pass rate)
- **Stage 1 (Discoverer)**: âœ… Successfully identifies TCCs with 85-95% confidence
- **Stage 2 (Auditor)**: âœ… Ranks TCCs as A/B/C-lines with proper scoring
- **Stage 3 (Modifier)**: âœ… Fixes structural issues (4/4 issues fixed in latest test)
- **Dependencies**: All packages installed (LangChain 1.0.5, LangGraph 1.0.3, LangSmith 0.1+)
- **DeepSeek Integration**: Configured and operational
- **Documentation**: Complete reference docs (100KB+ across 12+ files)
- **Error Handling**: Intelligent JSON parsing and schema validation with 0 retries
- **ğŸ†• LangSmith Observability**: âœ… Auto-tracing, performance metrics, cost estimation
- **ğŸ†• A/B Testing Framework**: âœ… Compare providers, prompts, and parameters
- **ğŸ†• Markdown Export** (v2.3.0): âœ… Professional reports + Mermaid visualization

### ğŸ‰ Recent Fixes (2025-11-13)
1. **Stage 2 JSON Parsing** (âœ… FIXED)
   - **Solution**: Enhanced `clean_json_response()` with bracket-stack matching algorithm
   - **Location**: `src/pipeline.py:155-215`
   - **Result**: Extracts first complete JSON object, ignoring trailing text

2. **Stage 2 Schema Validation** (âœ… FIXED)
   - **Solution**: Added field_validator for automatic type coercion (string â†’ list)
   - **Location**: `prompts/schemas.py:140-149`
   - **Result**: Handles LLM output format variations gracefully

3. **Stage 3 change_type Validation** (âœ… FIXED)
   - **Solution**: Extended allowed values and added normalization validator
   - **Location**: `prompts/schemas.py:260-274`
   - **Result**: Supports "remove" and "delete" operations with smart normalization

### ğŸ“Š Test Results Summary
```bash
# Unit Tests
pytest tests/ -v
# Result: 44 passed, 3 skipped (LLM integration tests)
# Status: âœ… 100% pass rate

# Complete Pipeline Test (End-to-End)
python -m src.cli analyze examples/golden/ç™¾å¦–_ep09_s01-s05.json
# Stage 1: âœ… Success (3 TCCs identified with 0.80-0.95 confidence)
# Stage 2: âœ… Success (A-line: TCC_01, B-line: TCC_02, C-line: TCC_03)
# Stage 3: âœ… Success (4/4 issues fixed: removed invalid scene references)
# Errors: 0
# Retries: 0
# Status: âœ… Production Ready
```

### ğŸ¯ System Status
**Status**: âœ… **Production Ready**
- All three stages operational
- Zero-retry success rate
- Comprehensive error handling
- Complete documentation
- Ready for real-world screenplay analysis

---

## Core Documentation

### 1. Project Overview
**File**: [`ref/project-overview.md`](ref/project-overview.md)

**Contents**:
- Project description and goals
- Key features and capabilities
- Technology stack
- Project status and metrics

**When to read**: Getting context about the project

---

### 2. Architecture
**File**: [`ref/architecture.md`](ref/architecture.md)

**Contents**:
- Director-Actor pattern design
- Three-stage pipeline explanation
- Data flow diagrams
- Component descriptions
- Error handling strategies
- LLM provider integration

**When to read**: Understanding system design, extending functionality

**Key sections**:
- Three-Stage Pipeline (lines 13-90)
- Data Flow (lines 92-125)
- Error Handling (lines 183-203)

---

### 3. Getting Started
**File**: [`ref/getting-started.md`](ref/getting-started.md)

**Contents**:
- Installation instructions
- API key setup
- Basic usage examples
- Configuration guide
- Common workflows
- Troubleshooting

**When to read**: First-time setup, configuration issues

**Key sections**:
- Quick Start (lines 5-35)
- API Key Setup (lines 60-96)
- Common Workflows (lines 182-261)

---

### 4. API Reference
**File**: [`ref/api-reference.md`](ref/api-reference.md)

**Contents**:
- Complete function signatures
- Parameter descriptions
- Return types
- Usage examples
- CLI commands
- Data models
- Validation functions
- Environment variables

**When to read**: Writing code, API integration

**Key sections**:
- Core Functions (lines 7-138)
- Data Models (lines 176-265)
- Validation Functions (lines 267-336)
- Calculation Functions (lines 338-407)

---

### 5. Testing Guide
**File**: [`ref/testing.md`](ref/testing.md)

**Contents**:
- Test structure overview
- Running tests
- Test categories (unit, integration, benchmarks)
- Test-driven development workflow
- Mocking strategies
- Coverage reporting
- CI/CD integration

**When to read**: Writing tests, debugging test failures

**Key sections**:
- Running Tests (lines 17-51)
- Schema Tests (lines 55-123)
- Golden Dataset Tests (lines 127-195)
- Benchmarks (lines 199-263)

---

### 6. Prompts Guide
**File**: [`ref/prompts-guide.md`](ref/prompts-guide.md)

**Contents**:
- Prompt architecture overview
- Stage-by-stage breakdown
- Prompt engineering best practices
- Customization strategies
- Testing prompts
- Versioning and migration

**When to read**: Modifying prompts, adding new stages

**Key sections**:
- Stage 1: Discoverer (lines 41-156)
- Stage 2: Auditor (lines 160-293)
- Stage 3: Modifier (lines 297-395)
- Customizing Prompts (lines 499-599)

---

## Important Files

### Source Code

#### Core Implementation
- **[`src/pipeline.py`](src/pipeline.py)** (18,954 bytes)
  - LangGraph pipeline
  - Actor implementations (DiscovererActor, AuditorActor, ModifierActor)
  - LLM provider integration
  - State management
  - **Start here for**: Pipeline logic, actor behavior

- **[`src/cli.py`](src/cli.py)** (11,500+ bytes)
  - Command-line interface
  - Commands: analyze, validate, benchmark, ab-test
  - **Start here for**: CLI usage, command implementation

- **[`src/monitoring.py`](src/monitoring.py)** (12,000+ bytes)
  - Metrics storage and analysis
  - Cost estimation utilities
  - Performance tracking
  - **Start here for**: Observability, cost tracking

- **[`src/ab_testing.py`](src/ab_testing.py)** (16,000+ bytes)
  - A/B testing framework
  - Variant comparison
  - Automated evaluation
  - **Start here for**: Testing different configurations

#### ğŸ†• Export System (v2.3.0)
- **[`src/exporters/markdown_exporter.py`](src/exporters/markdown_exporter.py)** (232 lines)
  - Markdown report generator
  - Jinja2 template rendering
  - Intelligent findings and recommendations
  - **Start here for**: Report generation, customization

- **[`src/exporters/mermaid_generator.py`](src/exporters/mermaid_generator.py)** (178 lines)
  - Mermaid flowchart generator
  - TCC relationship visualization
  - A/B/C line color coding
  - **Start here for**: Diagram generation, color schemes

- **[`templates/report_template.md.j2`](templates/report_template.md.j2)** (211 lines)
  - Jinja2 report template
  - Markdown structure definition
  - Customizable report layout
  - **Start here for**: Report template modification

#### Prompt System
- **[`prompts/schemas.py`](prompts/schemas.py)**
  - Pydantic models for all data structures
  - Validation functions
  - Calculation utilities
  - **Start here for**: Data models, validation logic

- **[`prompts/stage1_discoverer.md`](prompts/stage1_discoverer.md)**
  - Stage 1 prompt (TCC identification)
  - **Start here for**: Understanding TCC discovery logic

- **[`prompts/stage2_auditor.md`](prompts/stage2_auditor.md)**
  - Stage 2 prompt (A/B/C-line ranking)
  - **Start here for**: Understanding ranking criteria

- **[`prompts/stage3_modifier.md`](prompts/stage3_modifier.md)**
  - Stage 3 prompt (structural fixes)
  - **Start here for**: Understanding modification logic

- **[`prompts/README.md`](prompts/README.md)** (22,739 bytes)
  - Comprehensive prompt documentation
  - Usage patterns and examples
  - **Start here for**: Detailed prompt engineering guide

### Tests
- **[`tests/test_schemas.py`](tests/test_schemas.py)**
  - Unit tests for schemas and validators
  - **Start here for**: Schema testing patterns

- **[`tests/test_golden_dataset.py`](tests/test_golden_dataset.py)**
  - Integration tests with golden data
  - **Start here for**: End-to-end testing

- **[`benchmarks/run_benchmark.py`](benchmarks/run_benchmark.py)**
  - Performance benchmarking
  - **Start here for**: Performance testing

### Configuration
- **[`.env.example`](.env.example)** (1,374 bytes)
  - Environment variable template
  - API key configuration
  - **Copy to `.env`** and add your keys

- **[`requirements.txt`](requirements.txt)** (426 bytes)
  - Core dependencies
  - **Install with**: `pip install -r requirements.txt`

- **[`requirements-test.txt`](requirements-test.txt)** (392 bytes)
  - Test dependencies
  - **Install with**: `pip install -r requirements-test.txt`

### Documentation
- **[`README.md`](README.md)** (12,751 bytes, Chinese)
  - Main project documentation
  - Business context and theory
  - **Start here for**: Business understanding

- **[`USAGE.md`](USAGE.md)** (11,276 bytes)
  - Detailed usage guide
  - API examples
  - Troubleshooting
  - **Start here for**: Practical usage

- **[`README_DEEPSEEK.md`](README_DEEPSEEK.md)** (8,338 bytes)
  - DeepSeek integration guide
  - **Start here for**: DeepSeek setup

#### ğŸ†• Observability & Monitoring
- **[`docs/langsmith-quickstart.md`](docs/langsmith-quickstart.md)** (3 pages)
  - 5-minute LangSmith setup
  - **Start here for**: Quick observability setup

- **[`docs/langsmith-integration.md`](docs/langsmith-integration.md)** (15 pages)
  - Complete LangSmith guide
  - Configuration, usage, troubleshooting
  - **Start here for**: Full observability features

- **[`LANGSMITH_FEATURES.md`](docs/LANGSMITH_FEATURES.md)** (12 pages)
  - Technical implementation details
  - Architecture and design
  - **Start here for**: Understanding how tracing works

- **[`LANGSMITH_INTEGRATION_SUMMARY.md`](LANGSMITH_INTEGRATION_SUMMARY.md)** (11 pages)
  - Development summary
  - Completed features
  - **Start here for**: What was implemented

- **[`LANGSMITH_QUICKREF.md`](LANGSMITH_QUICKREF.md)** (1 page)
  - Quick reference card
  - **Start here for**: Command cheat sheet

#### ğŸ†• A/B Testing
- **[`docs/ab-testing-quickstart.md`](docs/ab-testing-quickstart.md)** (2 pages)
  - 3-minute A/B testing tutorial
  - **Start here for**: Quick A/B testing intro

- **[`docs/ab-testing-guide.md`](docs/ab-testing-guide.md)** (18 pages)
  - Complete A/B testing guide
  - Use cases, best practices, examples
  - **Start here for**: Comprehensive A/B testing

- **[`AB_TESTING_SUMMARY.md`](AB_TESTING_SUMMARY.md)** (10 pages)
  - A/B testing implementation summary
  - Features and architecture
  - **Start here for**: What was built

#### ğŸ†• Export & Reporting (v2.3.0)
- **[`docs/export-guide.md`](docs/export-guide.md)** (20+ pages)
  - Complete export functionality guide
  - Markdown report structure
  - Mermaid visualization usage
  - Report customization
  - **Start here for**: Generating and customizing reports

### Examples
- **[`examples/golden/`](examples/golden/)**
  - Golden dataset scripts for testing
  - Example: `ç™¾å¦–_ep09_s01-s05.json`
  - **Use for**: Testing and validation

---

## Common Tasks

### Task: Understand the System
1. Read [`ref/project-overview.md`](ref/project-overview.md)
2. Read [`ref/architecture.md`](ref/architecture.md)
3. Review [`README.md`](README.md) for business context

### Task: Set Up Development Environment
1. Follow [`ref/getting-started.md`](ref/getting-started.md) - Installation section
2. Configure API key using [`.env.example`](.env.example)
3. Run tests: `./run_tests.sh`

### Task: Analyze a Script
1. Check [`USAGE.md`](USAGE.md) - "Using the Pipeline" section
2. Use CLI: `python -m src.cli analyze script.json`
3. Or use API from [`ref/api-reference.md`](ref/api-reference.md)

### Task: Add New Feature
1. Read [`ref/architecture.md`](ref/architecture.md) - Extension Points
2. Review existing code in [`src/pipeline.py`](src/pipeline.py)
3. Follow TDD workflow from [`ref/testing.md`](ref/testing.md)
4. Update schemas in [`prompts/schemas.py`](prompts/schemas.py)

### Task: Modify Prompts
1. Read [`ref/prompts-guide.md`](ref/prompts-guide.md)
2. Edit files in [`prompts/`](prompts/) directory
3. Update [`prompts/schemas.py`](prompts/schemas.py) if needed
4. Test changes using [`tests/test_golden_dataset.py`](tests/test_golden_dataset.py)

### Task: Debug Test Failure
1. Check [`ref/testing.md`](ref/testing.md) - Troubleshooting section
2. Review test file: [`tests/test_schemas.py`](tests/test_schemas.py) or [`tests/test_golden_dataset.py`](tests/test_golden_dataset.py)
3. Run with verbose: `pytest tests/ -v -s`

### Task: Optimize Performance
1. Read [`ref/architecture.md`](ref/architecture.md) - Performance section
2. Run benchmarks: `python benchmarks/run_benchmark.py examples/golden`
3. Check [`USAGE.md`](USAGE.md) - Performance tips

### Task: Integrate New LLM Provider
1. Read [`ref/architecture.md`](ref/architecture.md) - LLM Provider Support
2. Edit [`src/pipeline.py`](src/pipeline.py) - `create_llm()` function
3. Update [`.env.example`](.env.example) with new variables
4. Test integration

### ğŸ†• Task: Enable LangSmith Tracing
1. Read [`docs/langsmith-quickstart.md`](docs/langsmith-quickstart.md) - 5-minute setup
2. Get API key from https://smith.langchain.com/
3. Update `.env`: `LANGCHAIN_TRACING_V2=true` and add API key
4. Run analysis - tracing is automatic

### ğŸ†• Task: Run A/B Test
1. Read [`docs/ab-testing-quickstart.md`](docs/ab-testing-quickstart.md) - 3-minute tutorial
2. Compare providers: `python -m src.cli ab-test script.json --providers deepseek,anthropic`
3. Compare temperatures: `python -m src.cli ab-test script.json --temperatures 0.0,0.7`
4. Review results in terminal and `ab_tests/` directory

### ğŸ†• Task: Analyze Performance Metrics
1. Run analysis with LangSmith enabled
2. Check terminal output for performance summary
3. View detailed metrics in `_metrics` field of result
4. Use `src/monitoring.py` for historical analysis

### ğŸ†• Task: Generate Report (v2.3.0)
1. Read [`docs/export-guide.md`](docs/export-guide.md) - Complete export guide
2. Analyze script with export flag: `python -m src.cli analyze script.json --export report.md`
3. View report in Markdown editor (VS Code, Obsidian, Typora)
4. Customize template in `templates/report_template.md.j2` if needed

---

## Key Concepts

### Theatrical Conflict Chain (TCC)
Independent narrative thread with super objective, forces, and evidence.
- **Details**: [`README.md`](README.md) lines 302-314
- **Schema**: [`prompts/schemas.py`](prompts/schemas.py)
- **Prompt**: [`prompts/stage1_discoverer.md`](prompts/stage1_discoverer.md)

### A/B/C-Line Classification
Ranking system for story importance (A=main, B=subplot, C=flavor).
- **Details**: [`README.md`](README.md) lines 54-68
- **Criteria**: [`ref/prompts-guide.md`](ref/prompts-guide.md) lines 164-244
- **Prompt**: [`prompts/stage2_auditor.md`](prompts/stage2_auditor.md)

### Setup-Payoff Chain
Cause-effect relationships between scenes.
- **Validation**: [`prompts/schemas.py`](prompts/schemas.py) - `validate_setup_payoff_integrity()`
- **Fixing**: [`prompts/stage3_modifier.md`](prompts/stage3_modifier.md)

### Director-Actor Pattern
LangGraph-based orchestration with specialized agents.
- **Details**: [`ref/architecture.md`](ref/architecture.md) lines 13-35
- **Implementation**: [`src/pipeline.py`](src/pipeline.py)

---

## Project Structure

```
.
â”œâ”€â”€ CLAUDE.md                    # This file - navigation guide
â”œâ”€â”€ README.md                    # Main documentation (Chinese)
â”œâ”€â”€ USAGE.md                     # Usage guide
â”œâ”€â”€ README_DEEPSEEK.md          # DeepSeek integration
â”‚
â”œâ”€â”€ ref/                         # Reference documentation
â”‚   â”œâ”€â”€ project-overview.md     # Project summary
â”‚   â”œâ”€â”€ architecture.md         # System design
â”‚   â”œâ”€â”€ getting-started.md      # Setup guide
â”‚   â”œâ”€â”€ api-reference.md        # API docs
â”‚   â”œâ”€â”€ testing.md              # Testing guide
â”‚   â””â”€â”€ prompts-guide.md        # Prompt engineering
â”‚
â”œâ”€â”€ src/                         # Source code
â”‚   â”œâ”€â”€ pipeline.py             # Core pipeline
â”‚   â”œâ”€â”€ cli.py                  # CLI interface
â”‚   â”œâ”€â”€ exporters/              # ğŸ†• Export system (v2.3.0)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ markdown_exporter.py # Markdown report generator
â”‚   â”‚   â””â”€â”€ mermaid_generator.py # Mermaid diagram generator
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ templates/                   # ğŸ†• Report templates (v2.3.0)
â”‚   â””â”€â”€ report_template.md.j2  # Jinja2 Markdown template
â”‚
â”œâ”€â”€ prompts/                     # Prompt system
â”‚   â”œâ”€â”€ stage1_discoverer.md    # Stage 1 prompt
â”‚   â”œâ”€â”€ stage2_auditor.md       # Stage 2 prompt
â”‚   â”œâ”€â”€ stage3_modifier.md      # Stage 3 prompt
â”‚   â”œâ”€â”€ schemas.py              # Data models
â”‚   â””â”€â”€ README.md               # Prompt guide
â”‚
â”œâ”€â”€ tests/                       # Test suites
â”‚   â”œâ”€â”€ test_schemas.py         # Unit tests
â”‚   â”œâ”€â”€ test_golden_dataset.py  # Integration tests
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ benchmarks/                  # Performance tests
â”‚   â””â”€â”€ run_benchmark.py
â”‚
â”œâ”€â”€ examples/                    # Sample data
â”‚   â””â”€â”€ golden/                 # Test datasets
â”‚
â”œâ”€â”€ docs/                        # Development docs
â”‚   â”œâ”€â”€ development-plan.md
â”‚   â””â”€â”€ test-script-conversion-plan.md
â”‚
â”œâ”€â”€ .env.example                 # Environment template
â”œâ”€â”€ requirements.txt             # Core dependencies
â”œâ”€â”€ requirements-test.txt        # Test dependencies
â”œâ”€â”€ pytest.ini                   # Pytest configuration
â””â”€â”€ run_tests.sh                # Test runner script
```

---

## Environment Setup Checklist

- [x] Python 3.8+ installed âœ…
- [x] Dependencies installed: `pip install -r requirements.txt` âœ…
  - LangChain 1.0.5
  - LangGraph 1.0.3
  - Pydantic, tenacity, etc.
- [x] `.env` file created from `.env.example` âœ…
- [x] API key added to `.env` (DeepSeek) âœ…
- [x] Tests passing: `pytest tests/ -v` âœ… (44/44 passed)
- [x] Stage 1 working: `python -m src.cli analyze examples/golden/ç™¾å¦–_ep09_s01-s05.json` âœ…
- [x] Stage 2 JSON parsing fixed âœ…
- [x] Stage 3 validation fixed âœ…
- [x] Complete 3-stage pipeline end-to-end test âœ… (0 errors, 0 retries)

---

## Development Workflow

### Standard Workflow
1. Read relevant documentation from `/ref`
2. Review existing code in `/src` or `/prompts`
3. Write tests first (see [`ref/testing.md`](ref/testing.md))
4. Implement feature
5. Run tests: `pytest tests/ -v`
6. Update documentation if needed

### Prompt Modification Workflow
1. Read [`ref/prompts-guide.md`](ref/prompts-guide.md)
2. Edit prompt file in [`prompts/`](prompts/)
3. Update [`prompts/schemas.py`](prompts/schemas.py) if schema changed
4. Test with golden dataset: `pytest tests/test_golden_dataset.py -v`
5. Update [`prompts/README.md`](prompts/README.md) with changes

### Adding New Feature Workflow
1. Design: Review [`ref/architecture.md`](ref/architecture.md)
2. Test: Write tests in [`tests/`](tests/)
3. Implement: Add code to [`src/`](src/)
4. Validate: Run full test suite
5. Document: Update [`ref/api-reference.md`](ref/api-reference.md)

---

## Troubleshooting Quick Reference

### Issue: API Key Error
- **Solution**: Check [`.env`](.env) file, see [`ref/getting-started.md`](ref/getting-started.md) lines 60-96

### Issue: Test Failure
- **Solution**: See [`ref/testing.md`](ref/testing.md) lines 533-567

### Issue: Validation Error
- **Solution**: Check [`prompts/schemas.py`](prompts/schemas.py), see [`ref/api-reference.md`](ref/api-reference.md) lines 440-457

### Issue: LLM Output Format Wrong
- **Solution**: See [`ref/prompts-guide.md`](ref/prompts-guide.md) lines 645-664

### Issue: Performance Slow
- **Solution**: See [`USAGE.md`](USAGE.md) lines 445-450, [`ref/architecture.md`](ref/architecture.md) lines 336-348

---

## External Resources

### LLM Providers
- DeepSeek: https://platform.deepseek.com/
- Anthropic Claude: https://console.anthropic.com/
- OpenAI: https://platform.openai.com/

### Technical Documentation
- LangChain: https://python.langchain.com/docs/
- LangGraph: https://langchain-ai.github.io/langgraph/
- LangSmith: https://docs.smith.langchain.com/
- Pydantic: https://docs.pydantic.dev/
- pytest: https://docs.pytest.org/

### Narrative Theory
- Robert McKee - "Story": Narrative structure principles
- "The Art of Dramatic Writing": Multi-line storytelling

---

## Version Information

**Project Version**: 2.3.0
**Prompt Version**: 2.1-Refactored + Auto-Merge
**Documentation Version**: 1.4 (Added Markdown export and Mermaid visualization)
**Last Updated**: 2025-11-13
**Latest Commit**: TBD (feat: add Markdown report export with Mermaid visualization)
**Completion Status**: 100% (All stages + observability + A/B testing + export - Production Ready)

---

## Contact & Support

- **Documentation Issues**: Review this file and linked docs
- **Code Issues**: Check [`tests/`](tests/) for examples
- **Prompt Issues**: See [`prompts/README.md`](prompts/README.md)
- **API Questions**: See [`ref/api-reference.md`](ref/api-reference.md)

---

## Notes for AI Assistants

### When Helping with This Project:

1. **Always start by reading**:
   - This file (CLAUDE.md) for navigation
   - [`ref/project-overview.md`](ref/project-overview.md) for context
   - [`ref/architecture.md`](ref/architecture.md) for design

2. **For specific tasks, consult**:
   - API questions â†’ [`ref/api-reference.md`](ref/api-reference.md)
   - Testing â†’ [`ref/testing.md`](ref/testing.md)
   - Prompts â†’ [`ref/prompts-guide.md`](ref/prompts-guide.md)
   - Setup â†’ [`ref/getting-started.md`](ref/getting-started.md)

3. **Code locations**:
   - Pipeline logic: [`src/pipeline.py`](src/pipeline.py)
   - Data models: [`prompts/schemas.py`](prompts/schemas.py)
   - Prompts: [`prompts/stage*.md`](prompts/)

4. **Testing approach**:
   - Unit tests: [`tests/test_schemas.py`](tests/test_schemas.py)
   - Integration: [`tests/test_golden_dataset.py`](tests/test_golden_dataset.py)
   - Examples: [`examples/golden/`](examples/golden/)

5. **This project uses**:
   - Python 3.8+
   - DeepSeek as default LLM (most cost-effective)
   - LangGraph for orchestration
   - Pydantic for validation
   - pytest for testing

---

**This file was generated by**: /initref command
**Purpose**: Provide quick navigation and context for AI-assisted development
**Maintenance**: Update when adding new features or documentation
